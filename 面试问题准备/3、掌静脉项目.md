
### **面试模拟**

**面试官：** “你好，感谢你今天能来参加面试。我看了你的简历，你这个基于TPU的掌静脉识别项目非常引人注目，看起来技术栈很全面。你能先从整体上介绍一下这个项目是做什么的，以及你在其中的核心职责是什么吗？”

你的回答：

“您好，谢谢您给我这次机会。这个项目是我们为第八届全国大学生集成电路创新创业大赛设计的‘基于TPU的掌静脉个人身份识别系统’。

我们系统的核心是基于算能CV1800B这块异构SoC芯片，它内置了RISC-V CPU和TPU，我们利用这个平台实现了一个完整的身份识别闭环。整个流程包括：通过BRC350 NIR摄像头采集用户的掌静脉图像，然后在Milk-V Duo开发板上进行一系列处理，包括图像的ROI提取和CLAHE增强。

我在其中的核心职责是 **AI硬件加速** 和 **软硬件协同开发**。具体来说：

- **AI硬件加速方面**：我负责将我们选用的MobileNet-V2神经网络模型部署到CV1800B的TPU上。这包括使用算能官方的TPU-MLIR工具链，完成从PyTorch模型到ONNX的转换，接着进行INT8非对称量化，最后编译生成可以在TPU上高效运行的`.cvimodel`文件。特别是在只有64MB内存的严苛限制下，成功实现了模型的部署与推理。
    
- **软硬件协同开发方面**：我负责驱动ST7789V液晶屏，具体是基于SPI总线协议进行通信，并且移植了LVGL这个轻量级图形库，为系统开发了一套完整的人机交互界面，用于实时显示识别结果和注册时的图像确认。
    

最终，我们的系统实现了在线注册和识别两大核心功能，在Poly-U测试集上准确率达到了99.69%，并获得了大赛华东赛区的二等奖。”

---

**面试官：** “非常出色。我们来深入聊聊技术细节。你提到了选用MobileNet-V2作为骨干网络，为什么会选择这个模型？它有什么特别适合你们这个项目的优点吗？”

你的回答：

“我们选择MobileNet-V2主要是基于它 硬件友好 的特性，非常适合我们这种硬件资源受限的边缘计算平台。具体来说，有以下几个关键原因：

- **参数量小，计算高效**：MobileNet-V2引入了‘倒残差结构’ (Inverted Residuals)，这种结构能够显著压缩模型的参数量，从而降低了对内存和算力的要求，这对于在只有64MB内存的Milk-V Duo上部署至关重要。
    
- **适合TPU推理**：这个模型的网络结构中只包含了1x1和3x3两种尺寸的卷积核。这两种卷积操作对于我们使用的CV1800B芯片上集成的TPU来说，计算效率非常高，能够最大化地利用硬件加速性能。
    
- **性能表现优异**：尽管它是一个轻量级网络，但在特征提取任务上依然保持了非常好的性能，足以满足我们掌静脉识别对精度的要求。”
    

---

**面试官：** “原来如此。你在简历中重点强调了使用TPU-MLIR工具链的部署过程。你能详细地描述一下，你是如何将一个训练好的PyTorch模型，一步步转换并量化成最终可以在TPU上运行的`.cvimodel`文件的吗？”

你的回答：

“当然可以。整个部署流程严格遵循了算能官方TPU-MLIR工具链的规范，主要分为三个核心步骤：

1. **模型格式转换 (Model Transform)**：首先，由于TPU-MLIR工具链不直接支持PyTorch，我需要先将我们训练好的`.pth`模型导出为通用的ONNX格式。然后，在配置好Sophgo镜像的Docker环境中，使用工具链中的 `model_transform.py` 脚本，将ONNX模型文件转换成TPU-MLIR能够理解的`.mlir`中间表示文件。
    
2. **生成量化校准表 (Calibration)**：为了将模型从FP32量化到INT8，我们需要一个校准过程来确定量化参数，以尽可能减少精度损失。我调用了 `run_calibration.py` 脚本，并准备了大约100到1000张有代表性的掌静脉图像作为校准数据集。脚本运行后，会生成一个 `${model_name}_cali_table` 文件，这个文件记录了模型中每一层激活值的分布信息，是后续INT8量化的关键依据。
    
3. **编译部署 (Model Deploy)**：最后一步是生成最终的可执行文件。我使用 `model_deploy.py` 脚本，将第一步生成的`.mlir`文件和第二步生成的校准表作为输入。在这个过程中，我指定了量化类型为 **INT8非对称量化**，因为这是我们使用的CV1800B芯片所支持的格式。编译完成后，就生成了最终的 `.cvimodel` 文件，这个文件可以直接加载到TPU上进行高效的硬件加速推理。
    

通过这套流程，我独立完成了从模型到可执行文件的完整部署。”

---

**面试官：** “描述得非常清晰。你在项目中遇到的最大技术挑战是什么？特别是简历里提到的64MB内存限制，你们是如何应对的？”

你的回答：

“我们遇到的最大技术挑战确实来自于 硬件资源的限制，尤其是64MB的内存。这个挑战具体体现在无法在内存中一次性加载完整的用户特征向量数据库。

我们的识别原理是在TPU上推理出待测图像的特征向量，然后与数据库中所有已注册用户的特征向量计算余弦距离。Poly-U数据集总共有500个类别（即500个用户）。当我们将这500个类别的所有特征向量全部加载到内存中时，会直接超出Milk-V Duo 64MB的内存上限，导致程序崩溃。

为了解决这个问题，并在比赛中成功演示我们的核心功能，我们采取了一个 **演示阶段的变通策略**：我们对片上内建的数据集规模进行了限制，在演示版本中，只加载了Poly-U数据集前20个类别的用户数据。这样做虽然限制了演示时的用户容量，但完整地验证了我们从图像采集、预处理、TPU推理到最终识别的全流程技术链路是通畅且有效的。

从长远来看，我们在项目报告的改进方向中也提到了，最终的解决方案是计划升级到拥有256MB内存的硬件平台，比如Duo-256M，来支持更大规模的用户数据库。”

---

**面试官：** “很棒的解决思路。我们来谈谈成果，99.69%的识别准确率非常高。请问这个指标是在什么条件下测得的？另外，我注意到你们的项目报告中提到了‘开集识别’，这和传统的分类有什么不同？你们的系统在这方面的表现如何？”

你的回答：

“您提的这个问题非常关键。

首先，**99.69%的准确率** 是在 **闭集识别 (closed-set recognition)** 的条件下，使用 **Poly-U数据集的测试集** 部分测得的。所谓闭集，就是指测试集中的用户类别和我们训练集中的用户类别是完全一致的。这主要验证了模型在我们已知用户范围内的识别精度。

而我们项目的核心创新点之一，正是实现了 **开集识别 (open-set recognition)**。传统的分类模型，比如直接用Softmax输出分类，只能判断一张图属于它学习过的N个类别中的哪一个。如果来了一个从未见过的用户（第N+1类），它依然会强制将其错误地归为已知的N类之一。这在实际应用中是不可接受的，因为系统无法拒绝非法用户。

我们的做法是：

- 不让模型直接输出分类，而是输出一个能代表掌静脉信息的 **特征向量**。
    
- 在识别时，我们计算待测图像特征向量与数据库中所有注册用户特征向量的 **余弦距离**。
    
- 通过设定一个阈值，如果最小的余弦距离仍然大于这个阈值，我们就判定该用户为‘未知用户’或‘非法用户’，从而实现了对训练集中未出现过的用户的拒绝功能。
    

为了验证开集识别的性能，我们使用了一个模型在训练中 **完全没有见过的数据集——VERA掌静脉数据集**。在这个纯粹的开集测试中，我们的系统依然达到了 **90.27%的识别精度**。这个结果有力地证明了我们算法有很好的泛化能力，能够识别训练集以外的用户，这对于商业化落地具有非常高的实用价值。”

---

**面试官：** “非常精彩的阐述，你对开集和闭集识别的理解很深刻。最后一个问题，关于你负责的软硬件协同部分，你是如何实现LVGL图形库在开发板上的移植，并与你的识别程序进行交互的？”

你的回答：

“好的。这部分工作主要分为硬件连接、驱动编译和软件应用三个层面。

1. **硬件连接与协议**：我首先根据Milk-V Duo和ST7789V液晶屏的引脚定义，通过面包板将它们连接起来。我们使用的是SPI（串行外设接口）协议进行通信，它由时钟线、数据输入/输出线和片选线组成，可以实现主控（Duo）和屏幕之间的高速数据传输。
    
2. **驱动移植**：为了让Linux系统能够识别并控制这块屏幕，我将ST7789V的驱动补丁编译进了Milk-V Duo的SDK镜像中。烧录这个修改后的镜像后，系统启动时就能成功加载屏幕驱动，并在`/dev`目录下创建对应的帧缓冲设备（frame buffer）。
    
3. **LVGL应用开发**：LVGL是一个非常适合嵌入式系统的轻量级、开源C语言图形库。我下载了官方提供的适用于Linux Frame Buffer的LVGL移植工程 `lv_port_linux_frame_buffer`。这个工程已经很好地封装了对底层帧缓冲的读写操作。我的工作是在这个工程的基础上进行二次开发：
    
    - 将我们的掌静脉识别主程序逻辑集成进去。
        
    - 调用LVGL提供的API来创建UI元素，比如文本标签和图像控件。
        
    - 当我们的识别程序得出结果时（例如，识别成功并返回用户名，或注册时需要显示ROI图像），主程序会调用相应的LVGL函数来更新屏幕上的内容。例如，在屏幕上显示“合法用户”的提示信息，或者在注册时显示当前采集到的掌静脉图像供用户确认。
        

通过这种方式，我将后端的AI推理算法和前端的显示交互有效地结合在了一起，实现了完整的用户体验闭环。”

---

**面试官：** “好的，非常感谢你的详细介绍。我们今天的面试就到这里。”